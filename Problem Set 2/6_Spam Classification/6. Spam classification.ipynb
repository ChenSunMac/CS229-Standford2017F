{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we will use the naive Bayes algorithm and an SVM to build a spam\n",
    "classifier.\n",
    "In recent years, spam on electronic newsgroups has been an increasing problem. Here, we’ll\n",
    "build a classifier to distinguish between “real” newsgroup messages, and spam messages.\n",
    "\n",
    "\n",
    "Using only the subject line and body of each message, we’ll learn to distinguish\n",
    "between the spam and non-spam.\n",
    "All the files for the problem are in http://cs229.stanford.edu/ps/ps2/spam_data.tgz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look at two sample spam emails in the files spam sample original*, and their preprocessed forms in the files spam sample preprocessed*. \n",
    "\n",
    "The first line in the preprocessed format is just the label and is not part of the message. The preprocessing ensures that only the message body and subject remain in the dataset; \n",
    "email addresses (EMAILADDR), web addresses (HTTPADDR), currency (DOLLAR) and numbers (NUMBER) were also replaced by the special tokens to allow them to be considered properly in the classification process. \n",
    "\n",
    "(In this problem, we’ll going to call the features “tokens” rather than “words,” since some of the features will correspond to special values like EMAILADDR. You don’t have to worry about the distinction.) \n",
    "\n",
    "The files news sample original and news sample preprocessed also give an example of a non-spam mail. The work to extract feature vectors out of the documents has also been done for you, so you can just load in the design matrices (called document-word matrices in text classification) containing all the data. In a document-word matrix, the i-th row represents the i-th document/email,and the j-th column represents the j-th distinct token. Thus, the (i, j)-entry of this matrix represents the number of occurrences of the j-th token in the i-th document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, we’ve chosen as our set of tokens considered (that is, as our vocabulary) only the medium frequency tokens. The intuition is that tokens that occur too often or too rarely do not have much classification value. (Examples tokens that occur very often are words like “the,” “and,” and “of,” which occur in so many emails and are sufficiently content-free that they aren’t worth modeling.) Also, words were stemmed using a standard stemming algorithm; basically, this means that “price,” “prices” and “priced” have all been replaced with “price,” so that they can be treated as the same word. For a list of the tokens used, see the file TOKENS LIST.\n",
    "\n",
    "Since the document-word matrix is extremely sparse (has lots of zero entries), we have stored it in our own efficient format to save space. You don’t have to worry about this format.\n",
    "\n",
    "For MATLAB: the file readMatrix.m provides the readMatrix function that reads in the document-word matrix and the correct class labels for the various documents. Code in nb train.m and nb test.m shows how readMatrix should be called. The documentation\n",
    "at the top of these two files will tell you all you need to know about the setup.\n",
    "\n",
    "For Python: the file nb.py provides the readMatrix function and starter code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

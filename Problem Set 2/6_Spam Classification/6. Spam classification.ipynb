{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we will use the naive Bayes algorithm and an SVM to build a spam\n",
    "classifier.\n",
    "In recent years, spam on electronic newsgroups has been an increasing problem. Here, we’ll\n",
    "build a classifier to distinguish between “real” newsgroup messages, and spam messages.\n",
    "\n",
    "\n",
    "Using only the subject line and body of each message, we’ll learn to distinguish\n",
    "between the spam and non-spam.\n",
    "All the files for the problem are in http://cs229.stanford.edu/ps/ps2/spam_data.tgz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look at two sample spam emails in the files spam sample original*, and their preprocessed forms in the files spam sample preprocessed*. \n",
    "\n",
    "The first line in the preprocessed format is just the label and is not part of the message. The preprocessing ensures that only the message body and subject remain in the dataset; \n",
    "email addresses (EMAILADDR), web addresses (HTTPADDR), currency (DOLLAR) and numbers (NUMBER) were also replaced by the special tokens to allow them to be considered properly in the classification process. \n",
    "\n",
    "(In this problem, we’ll going to call the features “tokens” rather than “words,” since some of the features will correspond to special values like EMAILADDR. You don’t have to worry about the distinction.) \n",
    "\n",
    "The files news sample original and news sample preprocessed also give an example of a non-spam mail. The work to extract feature vectors out of the documents has also been done for you, so you can just load in the design matrices (called document-word matrices in text classification) containing all the data. In a document-word matrix, the i-th row represents the i-th document/email,and the j-th column represents the j-th distinct token. Thus, the (i, j)-entry of this matrix represents the number of occurrences of the j-th token in the i-th document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, we’ve chosen as our set of tokens considered (that is, as our vocabulary) only the medium frequency tokens. The intuition is that tokens that occur too often or too rarely do not have much classification value. (Examples tokens that occur very often are words like “the,” “and,” and “of,” which occur in so many emails and are sufficiently content-free that they aren’t worth modeling.) Also, words were stemmed using a standard stemming algorithm; basically, this means that “price,” “prices” and “priced” have all been replaced with “price,” so that they can be treated as the same word. For a list of the tokens used, see the file TOKENS LIST.\n",
    "\n",
    "Since the document-word matrix is extremely sparse (has lots of zero entries), we have stored it in our own efficient format to save space. You don’t have to worry about this format.\n",
    "\n",
    "For MATLAB: the file readMatrix.m provides the readMatrix function that reads in the document-word matrix and the correct class labels for the various documents. Code in nb train.m and nb test.m shows how readMatrix should be called. The documentation\n",
    "at the top of these two files will tell you all you need to know about the setup.\n",
    "\n",
    "For Python: the file nb.py provides the readMatrix function and starter code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Event Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们把text classification 建模成 **多项式分布的事件模型**。 \n",
    "\n",
    "$x_i$：email里第i个单词的identity, 这个值在$\\{1,..., |V|\\}$ 当中选，其中 $|V|$ 是我们使用的 dictionary的最大值。\n",
    "\n",
    "$y$: whether the email is spam or not. \n",
    "\n",
    "那么一封 n 个单词的e-mail就可以被写成 $(x_1,...,x_n)$ 这样的n维向量。\n",
    "\n",
    "在**多项式分布的事件模型**里，我们假设每个单词是独立生成的，也就是说他们服从多项式分布(抛掷n次骰子)，所以说在这里我们假设$p(x_j|y)$处处相等，因为我们假设单词的出现不由位置决定。（和位置无关）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Set: $\\{x^{(i)}, y^{(i)}; i = 1,...,m\\}$ m个sample， where $x^{(i)} = (x_1^{(i)}, ...., x_{n_i}^{(i)})$.\n",
    "$n_i$ is the number of words in the i-th training sample.\n",
    "\n",
    "Then, the lokelihood oof the data is given by:\n",
    "$$\n",
    "\\textit{L}(\\phi, \\phi_{k|y = 0},\\phi_{k|y = 1}) = \\prod_{i = 1} ^{m} p(x^{(i)}, y^{(i)}) \\\\\n",
    " = \\prod_{i = 1}^{m} p (x_j^{(i)} | y; \\phi_{k|y = 0},\\phi_{k|y = 1}) p (y^{(i)}; \\phi_y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

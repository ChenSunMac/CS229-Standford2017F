{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Linear SVM 里，我们最后得到\n",
    "$$\n",
    "w^T x + b = \\sum_{i = 1}^m \\alpha_i y^{(i)} <x^{(i)}, x > + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "倘若映射到高位空间，我们需要求解 $\\phi(x)^T \\phi(x_{new})$。这里分两步，首先是控件转换，然后是高位空间中的内积运算，计算量会很大。如果把这两步合成一步，用$K(x, z) = \\phi(x)^T \\phi(z)$ 来直接简化两步计算。所以上式可以直接写成:\n",
    "$$\n",
    "w^T x + b = \\sum_{i = 1}^m \\alpha_i y^{(i)} K(x, z) + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM的一般流程： \n",
    "1. Constructing the Kernel Matrix $K \\in R^n \\times R^n \\rightarrow R$ ~(O(n^2))\n",
    "2. QP solve $\\alpha$ \n",
    "3. solve b with given $\\alpha$ and K\n",
    "4. return SVM for new input $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mercer Condition 可以用来判断一个函数能否作为核函数的条件，K应该满足对称性，半正定的性质，事实上，对称，半正定也是K可以作为核矩阵的充要条件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 Constructing Kernels:\n",
    "Let K1, K2 be kernels over $R^n \\times R^n$, let a $a\\in R^+$ be a positive real number, let $f: R^N \\rightarrow R $ be a real-valued function, let $\\phi: R^n \\rightarrow R^d$ be a function mapping from $R^n$ to $R^d$, let K3 be a kernel over $R^d \\times R^d$, and let p(x) a polynomial over x with positive coeffcients.\n",
    "\n",
    "For each of the functions K below, state whether it is necessarily a kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1). $K(x,z) = K_1(x,z) + K_2(x,z) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "u^TMu  = \\sum^{m}_{i,j} u_i u_j(K_1(x^{(i)}, x^{(j)}) + K_2(x^{(i)}, x^{(j)})) \\\\\n",
    " = \\sum^{m}_{i,j} u_i u_j(K_1(x^{(i)}, x^{(j)})) + \\sum^{m}_{i,j} u_i u_j(K_2(x^{(i)}, x^{(j)})) \\ge 0\n",
    "$$\n",
    "So yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2). $K(x,z) = aK_1(x,z) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "u^TMu  = a\\sum^{m}_{i,j} u_i u_j K_1(x^{(i)}, x^{(j)}) \\ge 0\n",
    "$$\n",
    "so yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3). $K(x,z) = K_1(x,z)  K_2(x,z) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $K_1$ and $K_2$ are kernels, it is straightforward to see that $K = K_1K_2$ is kernel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4). $K(x,z) = f(x)f(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no guarantee it's $\\ge 0$, so $K$ may not be a kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5).  $K(x,z) = K_3(\\phi(x), \\phi(z))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it appears that $\\phi$ doesn't matter as long as $K_3$ is a valid kernel. It's equivalent to transform all input first with $\\phi$ and then run SVM with  $K_3$ as a kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6).  $K(x,z) = p(K_1(x,z))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not necessarily, although $K_1$ is a valid kernel, p can performs another transformation on the original valid kernel to map it to an invalid one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 Kernelizing the Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we will consider a stochastic gradient descent-like implementation of the perceptron algorithm where each update to the parameters $\\theta$ is made using only one training example. However, unlike stochastic gradient descent, the perceptron algorithm will only make one pass through the entire training set. The update rule for this version of the perceptron algorithm is given by:\n",
    "$$\n",
    "\\theta^{(i+1)} : = \\theta^{(i)} + \\alpha y^{(i + 1)} x^{(i + 1)}, if~~~ h_\\theta(x^{(i + 1)})y^{(i+1)} < 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\theta(i)$ is the value of the parameters after the algorithm has seen the first i training\n",
    "examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** If $\\theta$ itself is of too high a dimension to be written explicityly, we would rather represented in the form of $K(\\theta, \\phi(x))$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In order to compute, we can always utilize the kernel trick.** Since the percetron uses hypotheses of the form:\n",
    "$$\n",
    "h_\\theta(x) = g(\\theta^T x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rewrite the mapping by the following:\n",
    "$$\n",
    "h_\\theta (x^{(i + 1)}) = g( (\\theta^{(i)})^T \\phi(x^{i+1}))\\\\\n",
    "= g(K(\\theta^{(i)}, \\phi(x^{(i + 1)})))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After the mapping, we want to rewrite the update rule, since we do not want to compute $\\theta$ explicitly.** That is, $K$ would be easier to compute. Hence we want to update $K$ instead of $\\theta$. We can do this by dot product both sides by $\\phi(x^{(i+1)})$\n",
    "$$\n",
    "\\phi(x^{(i+1)})\\theta^{(i+1)} : = \\phi(x^{(i+1)})\\theta^{(i)} + \\alpha y^{(i + 1)} \\phi(x^{(i+1)}) x^{(i + 1)}, if~~~ h_\\theta(x^{(i + 1)})y^{(i+1)} < 0\n",
    "$$\n",
    "Replace the dot product with kernel function, then we will get the result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
